{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "13356615119cf3a844474011b8fb40ae",
     "grade": false,
     "grade_id": "cell-3c98ddabe9e64f07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 1\n",
    "<br>\n",
    "<b>Deadline:</b> May 20, 2020 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "\n",
    "# Exercise 10.2. Conditional generation with PixelCNN\n",
    "\n",
    "The goal of this exercise is to do conditional generation with the PixelCNN model.\n",
    "The basic idea of the conditioning is described in Section 2.3 of [this paper](https://arxiv.org/pdf/1606.05328.pdf). However, we will use a much simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc0197afbbd904c2b006f24732e6e8a3",
     "grade": true,
     "grade_id": "cell-a54f4cac48b8daec",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b79b793e3771be4f29f1e582f8d5dfc6",
     "grade": false,
     "grade_id": "cell-6eeffe49baead231",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "767ca562df4a44a61a6bd37995d6c9c2",
     "grade": false,
     "grade_id": "cell-94c5742c02305758",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "In this exercise, we use standard MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c68960f7f54eb8281e78b61a10c0e5d3",
     "grade": false,
     "grade_id": "cell-532a4922e89ce5f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ab45f5c7bcbc4297f6e570397e77c09",
     "grade": false,
     "grade_id": "cell-72f0a284a46f0d97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = iter(trainloader).next()\n",
    "tools.plot_images(images[:8], ncol=4, cmap=plt.cm.Greys, clim=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff46bf85ab2c828cdc5995b4efe62dd2",
     "grade": false,
     "grade_id": "cell-a4de8a0f0588b4df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Masked convolutional layer\n",
    "\n",
    "In the cell below, copy the implementation of the `MaskedConv2d` from the PixelCNN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ead7030b2311293a95080875cbe940b0",
     "grade": false,
     "grade_id": "MaskedConv2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, blind_center=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_channels (int): Number of input channels.\n",
    "          out_channels (int): Number of output channels.\n",
    "          kernel_size (int): Kernel size similar to nn.Conv2d layer.\n",
    "          blind_center (bool): If True, the kernel has zero in the center.\n",
    "        \"\"\"\n",
    "        super(MaskedConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=int( (kernel_size-1)/2 ),\n",
    "            bias=False\n",
    "        )\n",
    " \n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.ones(\n",
    "                out_channels,\n",
    "                in_channels,\n",
    "                kernel_size,\n",
    "                kernel_size\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        height = width = kernel_size // 2\n",
    "        self.mask[:, :, height+1:] = 0\n",
    "        \n",
    "        if blind_center:\n",
    "            self.mask[:, :, height, width:] = 0\n",
    "        else:\n",
    "            self.mask[:, :, height , width + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x of shape (batch_size, in_channels, height, width): Input images.\n",
    "        \n",
    "        Returns:\n",
    "          y of shape (batch_size, out_channels, height, width): Output images.\n",
    "        \"\"\"\n",
    "        self.conv.weight.data = self.conv.weight.data * self.mask\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15abbe49f5f1e652b9f3d8ac4b3a5627",
     "grade": false,
     "grade_id": "cell-aaa542146c8ce33d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Conditional PixelCNN\n",
    "\n",
    "Conditional PixelCNN models allows to generate images of a desired class. This can be achieved by providing the desired class label to every layer of the PixelCNN model. In this notebook, we do it in the following way: the input of each masked convolutional layer is:\n",
    "$$\\mathbf{x} + \\mathbf{W} \\mathbf{h}$$\n",
    "where\n",
    "  * $\\mathbf{x}$ is the output of the previous layer\n",
    "  * $\\mathbf{h}$ is a 10-dimensional one-hot coded vector of the desired class\n",
    "  * $\\mathbf{W}$ is $c \\times 10$ matrix (parameter of a fully-connected layer), where $c$ is the number of input channels in the masked convolutional layer.\n",
    "\n",
    "You need to implement the conditional PixelCNN model in the cell below.\n",
    "\n",
    "Recommended architecture:\n",
    "* Use an architecture similar to the PixelCNN architecture in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fb7c7fdfdcf82854fc777d79ff1c698",
     "grade": false,
     "grade_id": "conditional_pixel_cnn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ConditionalPixelCNN(nn.Module):\n",
    "    def __init__(self, n_channels=64, kernel_size=7):\n",
    "        \"\"\"PixelCNN model for conditional generation.\"\"\"\n",
    "        super(ConditionalPixelCNN, self).__init__()\n",
    "        \n",
    "        self.model = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                MaskedConv2d(\n",
    "                    n_channels if i > 0 else 1,\n",
    "                    n_channels,\n",
    "                    kernel_size,\n",
    "                    blind_center=False if i>0 else True),\n",
    "                nn.BatchNorm2d(n_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for i in range(8)\n",
    "        ])\n",
    "        self.conv = nn.Conv2d(n_channels, 256, 1)\n",
    "        c = 10\n",
    "        self.W = nn.Linear(c, 1)\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"Compute logits of the conditional probabilities p(x_i|x_1, ..., x_{i-1}) of the PixelCNN model.\n",
    "        \n",
    "        Args:\n",
    "          x of shape (batch_size, 1, 28, 28): Tensor of input images.\n",
    "          labels of shape (batch_size): Long tensor of the desired classes of the generated samples.\n",
    "        \n",
    "        Returns:\n",
    "          logits of shape (batch_size, 256, 28, 28): Tensor of logits of the conditional probabilities\n",
    "                                                      for each pixel.\n",
    "        \n",
    "        NB: Do not use softmax nonlinearity after the last layer.\n",
    "        \"\"\"\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        one_hot = torch.zeros(batch_size, 10).to(labels.device)\n",
    "        one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "        b = self.W(one_hot).unsqueeze(2).unsqueeze(3).repeat(1, 1, height, width)\n",
    "        \n",
    "        for block in self.model:\n",
    "            x = block(x+b)\n",
    "        \n",
    "        y = self.conv(x+b)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3441d674d78988985cc12ac6c1ecdf4",
     "grade": false,
     "grade_id": "cell-6ceba5f92fe75b4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_ConditionalPixelCNN_shapes():\n",
    "    net = ConditionalPixelCNN(n_channels=64, kernel_size=7)\n",
    "\n",
    "    batch_size = 2\n",
    "    x = torch.ones(batch_size, 1, 28, 28)\n",
    "    labels = torch.zeros(batch_size, dtype=torch.long)\n",
    "    y = net(x, labels)\n",
    "    assert y.shape == torch.Size([batch_size, 256, 28, 28]), f\"Bad y.shape: {y.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_ConditionalPixelCNN_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66e1852e3ede50cfb5aac454a9e66142",
     "grade": false,
     "grade_id": "cell-33f0e5430af65349",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loss function for training conditional PixelCNN\n",
    "\n",
    "The `loss_fn()` function is identical to the `loss_fn()` from the PixelCNN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dbe63c296ea1dff830b1f7b1abac44b",
     "grade": false,
     "grade_id": "loss_fn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_fn(logits, x):\n",
    "    \"\"\"Compute loss of the conditional PixelCNN model. Please see PixelCNN.loss for more details.\n",
    "\n",
    "    Args:\n",
    "      logits of shape (batch_size, 256, 28, 28): Logits of the conditional probabilities\n",
    "                  p(x_i | x_1,...,x_{i-1}) of the 256 intensities of pixel x_i computed using all\n",
    "                  previous pixel value x_1,...,x_{i-1}.\n",
    "      x of shape (batch_size, 1, 28, 28): Images used to produce `generated_x`. The values of pixel\n",
    "                  intensities in x are between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "      loss: Scalar tensor which contains the value of the loss.\n",
    "    \"\"\"\n",
    "    x = (x*255).long().squeeze(1)\n",
    "    loss = F.cross_entropy(logits, x)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61130647ceae27ef8615e0975fe103ec",
     "grade": false,
     "grade_id": "cell-8e9892706a9d8986",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generation procedure\n",
    "\n",
    "The `generate()` function is *almost* identical to the `generate()` function from the PixelCNN notebook. It additionally receives the labels of the desired classes so that they can be used in the forward computations of the conditional PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4294be8877eb96082e905c7e2be1f00d",
     "grade": false,
     "grade_id": "generate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate(net, labels, image_size=(28, 28), device='cpu'):\n",
    "    \"\"\"Generate samples using a trained conditional PixelCNN model.\n",
    "    Note: use as device labels.device.\n",
    "\n",
    "    Args:\n",
    "      net: Conditional PixelCNN model.\n",
    "      labels of shape (n_samples): Long tensor of the desired classes of the generated samples.\n",
    "      image_size: Tuple of image size (height, width).\n",
    "      device:     Device to use.\n",
    "    \n",
    "    Returns:\n",
    "      samples of shape (n_samples, 1, height, width): Generated samples.\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    n_samples = labels.shape[0]\n",
    "    height, width = image_size\n",
    "    samples = torch.zeros(n_samples, 1, height, width).to(device)\n",
    "    \n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            out = net(samples, labels)\n",
    "            norm = F.softmax(out[:, :, h, w])\n",
    "            prob = torch.multinomial(norm, 1).float() / 255.0\n",
    "            samples[:, :, h, w] = prob\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4d31b69403ec5cf3f60e234968e9230",
     "grade": false,
     "grade_id": "cell-85f2af389e3b1c61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bba666955a640819c110ed3d0a77e110",
     "grade": false,
     "grade_id": "cell-d0de5c83645b3502",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalPixelCNN(\n",
       "  (model): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MaskedConv2d(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (W): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create network\n",
    "net = ConditionalPixelCNN(n_channels=64, kernel_size=7)\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f78714ea6a59cef933fc7faa429b76e3",
     "grade": false,
     "grade_id": "cell-481bb5e1d1ab36b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot generated samples for an untrained model\n",
    "# Note: generation on CPU may take a significant amount of time\n",
    "if not skip_training:\n",
    "    labels = torch.cat([torch.arange(10) for _ in range(12)], dim=0).to(device)\n",
    "    samples = generate(net, labels, device=device)\n",
    "    tools.plot_generated_samples(samples, ncol=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "846774ec9fe4295ef430127295832c96",
     "grade": false,
     "grade_id": "cell-cf49609428a403c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Training loop\n",
    "\n",
    "Implement the training loop in the cell below. The recommended hyperparameters:\n",
    "* Adam optimizer with learning rate 0.001\n",
    "* Number of epochs: 11.\n",
    "\n",
    "Hints:\n",
    "- The loss values are similar to the loss values in the PixelCNN notebook.\n",
    "- Please use this code to plot 120 generated samples after each epoch. This will allow you to track the training progress.\n",
    "```\n",
    "# Generate samples\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    labels = torch.cat([torch.arange(10) for _ in range(12)], dim=0).to(device)\n",
    "    samples = generate(net, labels, device=device)\n",
    "    tools.plot_generated_samples(samples, ncol=10)\n",
    "```\n",
    "- The generated images should be of great quality but you should definitely recognize the desired classes of the digits.\n",
    "- **Do not forget to set the model into the training mode by `net.train()` before training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eee972e75dfc82217d977fa7bab26b97",
     "grade": false,
     "grade_id": "cond_pixel_cnn_training_loop",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    train_loss = []\n",
    "    \n",
    "    for epoch in range(11):\n",
    "        running_loss = 0\n",
    "        for _, (image, label) in enumerate(trainloader):\n",
    "            net.train()\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = net(image, label)\n",
    "            loss = loss_fn(out, image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        \n",
    "        loss = running_loss / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "        print(f\"Epoch: {epoch+1} - Train Loss: {loss}\")\n",
    "        \n",
    "        # Generate samples\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            labels = torch.cat([torch.arange(10) for _ in range(12)], dim=0).to(device)\n",
    "            samples = generate(net, labels, device=device)\n",
    "            tools.plot_generated_samples(samples, ncol=10)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e425c9ed0879503db7963af897be4d9d",
     "grade": false,
     "grade_id": "cell-36010d91cd891307",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from 10_cond_pixelcnn.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "if not skip_training:\n",
    "    tools.save_model(net, '10_cond_pixelcnn.pth')\n",
    "else:\n",
    "    net = ConditionalPixelCNN(n_channels=64, kernel_size=7)\n",
    "    tools.load_model(net, '10_cond_pixelcnn.pth', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b4afefc2e69fd11b922010e72d32b44",
     "grade": false,
     "grade_id": "cell-560767c43e2ad560",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "150b38b82a2cb9b5b8934c5a3d33504a",
     "grade": false,
     "grade_id": "cell-fecbc19f46a95e57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if not skip_training:\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        labels = torch.cat([torch.arange(10) for _ in range(12)], dim=0).to(device)\n",
    "        samples = generate(net, labels, device=device)\n",
    "        tools.plot_generated_samples(samples, ncol=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01c348dee30c8dfa2663214dcebe1676",
     "grade": true,
     "grade_id": "cell-f0fc9ef5c12c97fc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests the training loss of the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73b516a0e610ef1cf013f531a2641884",
     "grade": false,
     "grade_id": "cell-25eed043aff8f44a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this notebook, we learned how to train a conditional PixelCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
